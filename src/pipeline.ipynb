{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adc00de-d10c-4530-a05f-3e4457c5c97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 00:46:34 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 00:46:36,066\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from model_and_sampling import get_vllm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3c26cd-c177-44f7-90e2-9d58bf8996ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_prompts import generate_prompts_sql, generate_bd_list, generate_sql_gt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157a81a4-52f5-4d0d-9a2e-ed98a5cc4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_and_save_answers import answers_process_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806730fa-5a9e-42f5-9014-60e2debd2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_sql import print_and_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7886e235-ec9e-4dd6-ac32-f533903ef4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_and_save_answers import answers_process_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ea500-3363-40fe-b017-dff716793496",
   "metadata": {},
   "source": [
    "## Загружаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7effd2a-d5b9-45d4-8d7e-75b44c1d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'model':\"/data/home/vkropoti/models/Qwen3-8B\",\n",
    "    'tensor_parallel_size':2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3819f63d-b9f2-4c3f-9630-c202f648ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 22:52:23 [config.py:689] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-02 22:52:24 [config.py:1713] Defaulting to use mp for distributed inference\n",
      "INFO 05-02 22:52:24 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 05-02 22:52:25 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/data/home/vkropoti/models/Qwen3-8B', speculative_config=None, tokenizer='/data/home/vkropoti/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/home/vkropoti/models/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-02 22:52:25 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-02 22:52:25 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_12b7bb13'), local_subscribe_addr='ipc:///tmp/326adfac-b2e1-4f18-9fa3-631a44cc6b86', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-02 22:52:26 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3d76413200>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:26 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a72af92d'), local_subscribe_addr='ipc:///tmp/bb79e833-b01e-440c-8801-a477b979e64b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-02 22:52:27 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3f23c04da0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:27 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_77b8b017'), local_subscribe_addr='ipc:///tmp/0a68530d-6658-4787-a3d7-7ee3fca0627d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:29 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "INFO 05-02 22:52:29 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:29 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 05-02 22:52:29 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:30 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/vkropoti/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 05-02 22:52:30 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/vkropoti/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:30 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_54ae4b61'), local_subscribe_addr='ipc:///tmp/626aa314-44e4-43f7-a400-604b6d976b75', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:30 [parallel_state.py:959] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:30 [parallel_state.py:959] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:30 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:30 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:30 [gpu_model_runner.py:1276] Starting to load model /data/home/vkropoti/models/Qwen3-8B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:30 [gpu_model_runner.py:1276] Starting to load model /data/home/vkropoti/models/Qwen3-8B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m WARNING 05-02 22:52:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m WARNING 05-02 22:52:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  7.06it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.75it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.10it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  1.90it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.13it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:32 [loader.py:458] Loading weights took 1.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:32 [loader.py:458] Loading weights took 1.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:33 [gpu_model_runner.py:1291] Model loading took 7.6394 GiB and 2.015484 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:33 [gpu_model_runner.py:1291] Model loading took 7.6394 GiB and 2.033307 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:416] Using cache directory: /home/vkropoti/.cache/vllm/torch_compile_cache/1e8b5e7fc5/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:426] Dynamo bytecode transform time: 8.09 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:416] Using cache directory: /home/vkropoti/.cache/vllm/torch_compile_cache/1e8b5e7fc5/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:426] Dynamo bytecode transform time: 8.19 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:41 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:52:47 [monitor.py:33] torch.compile takes 8.09 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:52:47 [monitor.py:33] torch.compile takes 8.19 s in total\n",
      "INFO 05-02 22:52:48 [kv_cache_utils.py:634] GPU KV cache size: 815,296 tokens\n",
      "INFO 05-02 22:52:48 [kv_cache_utils.py:637] Maximum concurrency for 5,000 tokens per request: 163.06x\n",
      "INFO 05-02 22:52:48 [kv_cache_utils.py:634] GPU KV cache size: 815,296 tokens\n",
      "INFO 05-02 22:52:48 [kv_cache_utils.py:637] Maximum concurrency for 5,000 tokens per request: 163.06x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:53:07 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "INFO 05-02 22:53:07 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1877065)\u001b[0;0m INFO 05-02 22:53:07 [gpu_model_runner.py:1626] Graph capturing finished in 19 secs, took 0.70 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1877039)\u001b[0;0m INFO 05-02 22:53:07 [gpu_model_runner.py:1626] Graph capturing finished in 19 secs, took 0.70 GiB\n",
      "INFO 05-02 22:53:07 [core.py:163] init engine (profile, create kv cache, warmup model) took 34.42 seconds\n",
      "INFO 05-02 22:53:07 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm, sampling_params = get_vllm_model(model=model_config['model'],\n",
    "                                      tensor_parallel_size=model_config['tensor_parallel_size']\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebb38f-fd6e-459a-a428-51ff84202647",
   "metadata": {},
   "source": [
    "## Получаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41e95c5-c87b-4419-b738-a5a27b61acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are helpfull assistant solving text to sql. Carefully answer the question and add <|im_end|> and the end of it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96942c38-d486-4d53-afd3-0c5567ffd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_config = {\n",
    "    'path_dev_json':'/home/vkropoti/vllm/dev.json',\n",
    "    'path_sql_dbs': \"/data/home/vkropoti/sql_data/dev_databases/\",\n",
    "    'use_reasoning':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26288e8-2724-4cfd-bb40-dd93d65b2561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 642 ms, sys: 146 ms, total: 788 ms\n",
      "Wall time: 790 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_prompts = generate_prompts_sql(path_dev_json=prompts_config['path_dev_json'],\n",
    "                                    path_sql_dbs=prompts_config['path_sql_dbs'],\n",
    "                                     model_name=model_config['model'],\n",
    "                                     system_prompt=system_prompt,\n",
    "                                     use_reasoning = prompts_config['use_reasoning']\n",
    "                                    )\n",
    "bd_list = generate_bd_list(path_dev_json=prompts_config['path_dev_json'])\n",
    "sql_gt_list = generate_sql_gt_list(path_dev_json=prompts_config['path_dev_json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a3a74-4229-487e-8f59-738870591a99",
   "metadata": {},
   "source": [
    "## Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16377cc3-5ad0-4020-90d5-703a30a7dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1534/1534 [00:09<00:00, 157.90it/s, est. speed input: 163840.08 toks/s, output: 6987.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 s, sys: 50.6 ms, total: 3.27 s\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = llm.generate(batch_prompts,sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59a9f4-6636-4563-96b1-1700906cee59",
   "metadata": {},
   "source": [
    "## Обработка и сохранение ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cddb660-c5da-4659-962b-4b060f6ea94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_think количество ошибок:  0\n",
      "extract_sql количество ошибок:  0\n"
     ]
    }
   ],
   "source": [
    "sql_predict = answers_process_pipeline(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf940a01-0315-449a-9c9d-bf99846a5f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT MAX(`Percent (%) Eligible Free (K-12)`) FROM frpm WHERE `County Name` = 'Alameda';\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c6135-ceac-404b-8583-ef9c51957f88",
   "metadata": {},
   "source": [
    "## Подсчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67fb1d0-fb25-4fce-9b18-e2e766e52b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sql_dbs = \"/data/home/vkropoti/sql_data/dev_databases/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0972957-b6b5-4dba-b774-ea7a3a6ebff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3-8b Финальный результат EX: 0.452\n",
      "Qwen3-8b gроцент запросов, которые успешно выполнились: 76.59%\n",
      "Qwen3-8b Mini DEV Финальный результат EX: 0.414\n",
      "Qwen3-8b процент запросов, которые успешно выполнились Mini DEV: 76.00%\n",
      "CPU times: user 1min 58s, sys: 1min 40s, total: 3min 38s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print_and_save(\"Qwen3-8b\",path_sql_dbs,bd_list,sql_gt_list,sql_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93b90030-757b-4c3f-823b-87d9deadc761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f49c9df2d50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/home/vkropoti/vllm/lib64/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "func_timeout.dafunc.FunctionTimedOut1840702833704239227: Function sqlite_operation (args=()) (kwargs={}) timed out after 5.000000 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3-8b Финальный результат EX: 0.452\n",
      "Qwen3-8b gроцент запросов, которые успешно выполнились: 76.65%\n",
      "Qwen3-8b Mini DEV Финальный результат EX: 0.414\n",
      "Qwen3-8b процент запросов, которые успешно выполнились Mini DEV: 76.20%\n",
      "CPU times: user 2min 2s, sys: 1min 35s, total: 3min 37s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print_and_save(\"Qwen3-8b\",path_sql_dbs,bd_list,sql_gt_list,sql_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd3f576-a0ee-4ff5-af6f-d9e9766b0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_and_save??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990ff37d-946d-4794-a90f-32725faf9516",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_think_train_configs\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline, get_batch_example\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/src/pipeline.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_and_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_sampling_func\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_prompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_prompts_sql, generate_bd_list, generate_sql_gt_list\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocess_and_save_answers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m answers_process_pipeline\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from config import create_think_train_configs\n",
    "from pipeline import pipeline, get_batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1320537a-ca25-4349-bb77-6ba8c8809815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_json(dir):\n",
    "    with open(dir, \"r\") as j:\n",
    "        contents = json.loads(j.read())\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1254ec-47ac-4d93-a575-ad1f4390e075",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m f = \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/data/home/vkropoti/sql_data/train/train.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_json\u001b[39m\u001b[34m(dir)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json\u001b[39m(\u001b[38;5;28mdir\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mdir\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m j:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         contents = \u001b[43mjson\u001b[49m.loads(j.read())\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m contents\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "f = load_json(\"/data/home/vkropoti/sql_data/train/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f36f77-d278-47ba-954e-dc586ba5954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config, prompts_config, save_config = create_think_train_configs(model,arr_seeds[0],arr_versions[0])\n",
    "    llm = get_vllm_model(model=model_config['model'],\n",
    "                                      tensor_parallel_size=model_config['tensor_parallel_size'],\n",
    "                                          max_model_len=model_config['max_model_len'],\n",
    "                                          gpu_memory_utilization=model_config['gpu_memory_utilization'],\n",
    "                                          trust_remote_code=model_config['trust_remote_code'],\n",
    "                                     )\n",
    "    print(\"Модель успешно загружена\")\n",
    "    print(\"model_config\",model_config)\n",
    "    print()\n",
    "    print(\"prompts_config\",prompts_config)\n",
    "    print()\n",
    "    print(\"save_config\",save_config)\n",
    "    print()\n",
    "    print(\"Модель успешно загружена\")\n",
    "    batch_example, max_tokens = get_batch_example(model_config, prompts_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
